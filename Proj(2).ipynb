{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9KgORN1jNlG",
        "outputId": "60138093-362c-4e6e-a5cd-2a436dd1fc9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "import string\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset=pd.read_csv('/content/ASVP_ESD_text(1-1500).csv')"
      ],
      "metadata": {
        "id": "cReoiMZNXrO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def split_train_test(df, test_size=0.2, random_state=None):\n",
        "\n",
        "    train_df, test_df = train_test_split(df, test_size=test_size, random_state=random_state)\n",
        "    return train_df, test_df\n"
      ],
      "metadata": {
        "id": "eL4A-DUEqjpB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df,test_df=split_train_test(dataset)"
      ],
      "metadata": {
        "id": "lCHtapfJq6dx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSg1KhiOroY4",
        "outputId": "94980dd4-593b-41c2-cd73-5c33a8e277c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                      Emotion                     Transcription  Label\n",
            "2    disgust/dislike/contempt                            Play.       0\n",
            "261  disgust/dislike/contempt    Is water it tastes like shit.       0\n",
            "129              neutral/calm       Happy saint patrick's day.       1\n",
            "155              neutral/calm  How many smaller ants in death.       1\n",
            "370                   excited                          Cheers.       1\n",
            "..                        ...                               ...    ...\n",
            "9                     excited                            Good.       1\n",
            "31   disgust/dislike/contempt               Where do you like.       0\n",
            "57         happy/laugh/gaggle                       Sorry for.       1\n",
            "375              boredom/sigh         On the side and once we.       0\n",
            "296  disgust/dislike/contempt         Skip it through a straw.       0\n",
            "\n",
            "[324 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.reset_index(drop=True,inplace=True)\n",
        "test_df.reset_index(drop=True,inplace=True)"
      ],
      "metadata": {
        "id": "_67KtkTlD5Kr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewmTGdwpOt3e",
        "outputId": "6b325db2-d9d5-4c62-db94-3ff20a867b2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(324, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def stemmer(dataset):\n",
        "    corpus = []\n",
        "    # Check if it's a single row DataFrame or not\n",
        "    if isinstance(dataset, pd.DataFrame) and len(dataset) == 1:\n",
        "        review = re.sub('[^a-zA-Z]', ' ', dataset.iloc[0]['Transcription'])  # Access the value of the 'Transcription' column\n",
        "        review = review.lower()\n",
        "        review = review.split()\n",
        "        ps = PorterStemmer()\n",
        "        all_stopwords = stopwords.words('english')\n",
        "        all_stopwords.remove('not')\n",
        "        review = [ps.stem(word) for word in review if not word in set(all_stopwords)]\n",
        "        review = ' '.join(review)\n",
        "        corpus.append(review)\n",
        "    else:\n",
        "        for i in range(len(dataset)):\n",
        "            review = re.sub('[^a-zA-Z]', ' ', dataset['Transcription'][i])\n",
        "            review = review.lower()\n",
        "            review = review.split()\n",
        "            ps = PorterStemmer()\n",
        "            all_stopwords = stopwords.words('english')\n",
        "            all_stopwords.remove('not')\n",
        "            review = [ps.stem(word) for word in review if not word in set(all_stopwords)]\n",
        "            review = ' '.join(review)\n",
        "            corpus.append(review)\n",
        "    return corpus"
      ],
      "metadata": {
        "id": "yWQkS1uEYEk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_freqs(df, ys):\n",
        "    \"\"\"Build frequencies.\n",
        "    Input:\n",
        "        tweets: a list of tweets where each tweet is a string\n",
        "        ys: an m x 1 array with the sentiment label of each tweet\n",
        "            (either 0 or 1)\n",
        "    Output:\n",
        "        freqs: a dictionary mapping each (word, sentiment) pair to its\n",
        "        frequency\n",
        "    \"\"\"\n",
        "    # Convert np array to list since zip needs an iterable.\n",
        "    # The squeeze is necessary or the list ends up with one element.\n",
        "    # Also note that this is just a NOP if ys is already a list.\n",
        "    corpus=stemmer(df)\n",
        "    tokenized_corpus = [word for sentence in corpus for word in sentence.split()]\n",
        "    tweets=tokenized_corpus\n",
        "    yslist = np.squeeze(ys).tolist()\n",
        "\n",
        "    # Start with an empty dictionary and populate it by looping over all tweets\n",
        "    # and over all processed words in each tweet.\n",
        "    freqs = {}\n",
        "    for y, tweet in zip(yslist, tweets):\n",
        "        # Tokenize the tweet into words\n",
        "        words = word_tokenize(tweet)\n",
        "        for word in words:\n",
        "            pair = (word, y)\n",
        "            if pair in freqs:\n",
        "                freqs[pair] += 1\n",
        "            else:\n",
        "                freqs[pair] = 1\n",
        "\n",
        "    return freqs\n"
      ],
      "metadata": {
        "id": "6OJA1eEBQ8dU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(df, freqs):\n",
        "    '''\n",
        "    Input:\n",
        "        tweet: a list of words for one tweet\n",
        "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
        "    Output:\n",
        "        x: a feature vector of dimension (1,3)\n",
        "    '''\n",
        "    corpus=stemmer(df)\n",
        "    tokenized_corpus = [word for sentence in corpus for word in sentence.split()]\n",
        "    word_l = tokenized_corpus\n",
        "\n",
        "    # 3 elements for [bias, positive, negative] counts\n",
        "    x = np.zeros(3)\n",
        "\n",
        "    # bias term is set to 1\n",
        "    x[0] = 1\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "\n",
        "    # loop through each word in the list of words\n",
        "    for word in word_l:\n",
        "\n",
        "        # increment the word count for the positive label 1\n",
        "        x[1] += freqs.get((word, 1.0),0)\n",
        "\n",
        "\n",
        "        # increment the word count for the negative label 0\n",
        "        x[2] +=  freqs.get((word, 0.0),0)\n",
        "\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    x = x[None, :]  # adding batch dimension for further processing\n",
        "    assert(x.shape == (1, 3))\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "OE2vh23hnRlW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train=train_df['Label'].values\n",
        "y_test=test_df['Label'].values\n",
        "freqs=build_freqs(train_df,y_train)"
      ],
      "metadata": {
        "id": "IFQZVLBLqNMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(freqs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hs6uf9jsCsv",
        "outputId": "85f919e3-a7a9-4ecd-8907-483f0bdde8b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{('play', 0): 2, ('water', 0): 1, ('tast', 1): 2, ('like', 1): 2, ('shit', 1): 2, ('happi', 0): 1, ('saint', 1): 1, ('patrick', 1): 1, ('day', 0): 1, ('mani', 1): 1, ('smaller', 0): 1, ('ant', 0): 1, ('death', 0): 1, ('cheer', 0): 1, ('treat', 0): 1, ('women', 0): 2, ('like', 0): 1, ('piec', 1): 2, ('garbag', 1): 1, ('buy', 1): 1, ('even', 0): 1, ('go', 1): 2, ('secret', 1): 2, ('exercis', 1): 1, ('eat', 1): 1, ('health', 1): 1, ('play', 1): 4, ('song', 1): 2, ('navig', 1): 1, ('voic', 1): 1, ('love', 0): 2, ('best', 0): 1, ('oh', 0): 3, ('go', 0): 2, ('way', 0): 3, ('want', 0): 1, ('matter', 0): 1, ('hard', 0): 1, ('tri', 1): 1, ('never', 1): 1, ('enough', 0): 1, ('everyth', 1): 1, ('end', 0): 1, ('destroy', 0): 1, ('pointless', 1): 1, ('social', 0): 1, ('wonder', 0): 1, ('chair', 1): 1, ('synagogu', 0): 1, ('cologn', 0): 1, ('grow', 1): 1, ('lower', 1): 1, ('level', 1): 1, ('mamma', 1): 1, ('mia', 1): 1, ('get', 1): 1, ('dad', 1): 1, ('bologna', 1): 1, ('sweet', 0): 1, ('weird', 1): 1, ('textur', 1): 1, ('peanut', 0): 2, ('butter', 1): 2, ('jelli', 0): 1, ('year', 1): 1, ('quit', 0): 1, ('walmart', 1): 1, ('chang', 1): 1, ('main', 1): 1, ('tell', 0): 2, ('see', 0): 1, ('time', 1): 2, ('ye', 0): 1, ('not', 1): 1, ('fuck', 0): 3, ('drunk', 0): 1, ('god', 1): 3, ('amaz', 1): 1, ('murder', 1): 1, ('sharon', 0): 1, ('ye', 1): 1, ('fuc', 0): 1, ('marriag', 1): 1, ('nice', 1): 1, ('youtub', 0): 1, ('fellow', 1): 1, ('follow', 1): 1, ('want', 1): 2, ('come', 1): 2, ('home', 0): 1, ('sold', 1): 1, ('busi', 0): 1, ('padlock', 0): 1, ('door', 1): 1, ('lot', 1): 1, ('thing', 0): 1, ('scatter', 1): 1, ('holi', 1): 1, ('famili', 0): 1, ('know', 1): 2, ('look', 1): 1, ('stadium', 1): 1, ('oh', 1): 2, ('set', 1): 2, ('volum', 1): 2, ('solomon', 1): 1, ('northup', 0): 1, ('staunton', 1): 1, ('warrior', 1): 1, ('must', 1): 1, ('pineappl', 0): 1, ('ave', 1): 1, ('sorri', 1): 4, ('stop', 0): 4, ('redefin', 1): 2, ('bassist', 1): 1, ('understand', 1): 1, ('confeder', 1): 1, ('solv', 1): 1, ('problem', 0): 2, ('first', 0): 1, ('understand', 0): 1, ('free', 1): 1, ('suck', 0): 1, ('roll', 1): 1, ('man', 1): 1, ('show', 1): 1, ('alarm', 1): 1, ('white', 1): 1, ('jesu', 1): 1, ('amaz', 0): 1, ('someth', 1): 1, ('good', 1): 1, ('somewher', 1): 1, ('discrimin', 0): 1, ('youtub', 1): 1, ('pleas', 0): 2, ('stop', 1): 8, ('happi', 1): 1, ('valentin', 0): 1, ('day', 1): 1, ('bulshit', 0): 1, ('job', 0): 1, ('result', 1): 1, ('scientif', 1): 1, ('data', 0): 1, ('wrong', 1): 1, ('big', 0): 1, ('news', 1): 2, ('naruto', 0): 1, ('album', 0): 1, ('thank', 1): 1, ('fuck', 1): 3, ('good', 0): 1, ('morn', 0): 1, ('sir', 1): 1, ('help', 1): 1, ('paper', 0): 1, ('appoint', 0): 1, ('download', 0): 1, ('say', 1): 1, ('think', 1): 1, ('mean', 1): 1, ('weak', 0): 1, ('hear', 0): 1, ('gun', 0): 1, ('flat', 0): 1, ('crazi', 1): 1, ('dalla', 0): 1, ('counti', 0): 1, ('build', 1): 1, ('one', 1): 1, ('commiss', 1): 1, ('million', 1): 1, ('dollar', 0): 1, ('radio', 0): 1, ('miss', 1): 1, ('say', 0): 1, ('said', 0): 1, ('degre', 0): 1, ('turn', 0): 1, ('announc', 1): 2, ('big', 1): 1, ('asymptomat', 1): 1, ('peopl', 1): 1, ('increas', 1): 1, ('oldest', 1): 1, ('meet', 1): 1, ('boy', 1): 1, ('c', 0): 1, ('demand', 1): 1, ('pharmaci', 0): 1, ('short', 1): 1, ('call', 1): 1, ('attorney', 0): 1, ('gener', 1): 1, ('phil', 0): 1, ('wiser', 0): 1, ('not', 0): 1, ('oppos', 1): 1, ('petit', 1): 1, ('mama', 0): 1, ('la', 1): 1, ('familia', 0): 1, ('seen', 1): 1, ('realli', 0): 2, ('shine', 1): 1, ('love', 1): 1, ('us', 1): 1, ('back', 1): 1, ('togeth', 1): 1, ('googl', 0): 1, ('hello', 1): 1, ('pleas', 1): 1, ('help', 0): 1, ('one', 0): 1, ('biggest', 1): 1, ('wish', 0): 1, ('lil', 1): 1, ('dab', 0): 1, ('mayonnais', 1): 1, ('make', 0): 1, ('hate', 1): 2, ('oat', 1): 1, ('unbeliev', 0): 1, ('unit', 0): 1, ('state', 1): 1, ('know', 0): 1, ('yeehaw', 1): 1, ('surround', 1): 1, ('two', 0): 1, ('thiev', 0): 1, ('left', 1): 1, ('right', 1): 1, ('european', 0): 1, ('chines', 1): 1, ('bu', 1): 1, ('hello', 0): 1, ('fact', 0): 1, ('admit', 1): 1, ('great', 1): 1, ('grandfath', 1): 1, ('own', 1): 1, ('slave', 0): 1, ('yet', 1): 1, ('part', 1): 1, ('histori', 0): 1, ('part', 0): 1, ('famili', 1): 1, ('heritag', 1): 1, ('make', 1): 1, ('okay', 0): 1, ('clearli', 0): 1, ('silver', 1): 1, ('boston', 1): 1, ('prive', 1): 1, ('monster', 0): 1, ('machin', 1): 1, ('possess', 1): 1, ('taint', 1): 1, ('item', 1): 1, ('proce', 1): 1, ('crime', 1): 1, ('act', 1): 1, ('ask', 1): 1, ('siri', 1): 1, ('golden', 0): 1, ('pay', 1): 1, ('attent', 0): 1, ('class', 0): 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = np.zeros((len(train_df), 3))\n",
        "for i in range(len(train_df)):\n",
        "    X_train[i, :]= extract_features(train_df.iloc[[i]], freqs)"
      ],
      "metadata": {
        "id": "OwCnh41QojvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = np.zeros((len(test_df), 3))\n",
        "for i in range(len(test_df)):\n",
        "    X_test[i, :]= extract_features(test_df.iloc[[i]], freqs)"
      ],
      "metadata": {
        "id": "zEU4t3btX7Vu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHbywHGarTyR",
        "outputId": "9c2d4d5a-ba4b-4be1-dcdd-092b711c2c7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1.  4.  2.]\n",
            " [ 1.  6.  2.]\n",
            " [ 1.  4.  2.]\n",
            " [ 1.  1.  3.]\n",
            " [ 1.  0.  1.]\n",
            " [ 1.  8.  9.]\n",
            " [ 1.  7.  0.]\n",
            " [ 1.  6.  2.]\n",
            " [ 1.  2.  0.]\n",
            " [ 1.  3.  3.]\n",
            " [ 1.  2.  3.]\n",
            " [ 1.  8. 11.]\n",
            " [ 1.  0.  0.]\n",
            " [ 1.  1.  4.]\n",
            " [ 1.  1.  0.]\n",
            " [ 1.  2.  0.]\n",
            " [ 1.  0.  0.]\n",
            " [ 1.  2.  0.]\n",
            " [ 1.  2.  0.]\n",
            " [ 1.  8.  6.]\n",
            " [ 1.  0.  1.]\n",
            " [ 1.  5.  0.]\n",
            " [ 1.  0.  3.]\n",
            " [ 1.  7.  8.]\n",
            " [ 1.  6.  4.]\n",
            " [ 1.  1.  0.]\n",
            " [ 1.  1.  2.]\n",
            " [ 1.  0.  1.]\n",
            " [ 1.  2.  6.]\n",
            " [ 1.  7.  3.]\n",
            " [ 1.  8.  5.]\n",
            " [ 1.  2.  1.]\n",
            " [ 1.  2.  1.]\n",
            " [ 1.  2.  0.]\n",
            " [ 1.  5.  3.]\n",
            " [ 1.  6.  1.]\n",
            " [ 1.  1.  0.]\n",
            " [ 1.  1.  0.]\n",
            " [ 1.  1.  1.]\n",
            " [ 1.  8.  0.]\n",
            " [ 1.  8.  4.]\n",
            " [ 1.  9.  5.]\n",
            " [ 1.  3.  0.]\n",
            " [ 1.  0.  1.]\n",
            " [ 1.  2.  0.]\n",
            " [ 1.  2.  0.]\n",
            " [ 1.  2.  0.]\n",
            " [ 1.  1.  1.]\n",
            " [ 1.  3.  1.]\n",
            " [ 1.  4.  0.]\n",
            " [ 1.  0.  1.]\n",
            " [ 1.  3.  3.]\n",
            " [ 1. 74. 40.]\n",
            " [ 1.  2.  3.]\n",
            " [ 1.  2.  2.]\n",
            " [ 1.  5.  5.]\n",
            " [ 1.  6.  3.]\n",
            " [ 1.  0.  0.]\n",
            " [ 1.  0.  1.]\n",
            " [ 1.  4.  3.]\n",
            " [ 1.  1.  0.]\n",
            " [ 1. 14. 10.]\n",
            " [ 1.  0.  0.]\n",
            " [ 1.  2.  2.]\n",
            " [ 1.  1.  1.]\n",
            " [ 1.  0.  1.]\n",
            " [ 1.  0.  1.]\n",
            " [ 1.  0.  1.]\n",
            " [ 1.  2.  0.]\n",
            " [ 1.  1.  1.]\n",
            " [ 1.  1.  0.]\n",
            " [ 1.  2.  3.]\n",
            " [ 1.  1.  1.]\n",
            " [ 1.  3.  5.]\n",
            " [ 1.  1.  2.]\n",
            " [ 1.  2.  1.]\n",
            " [ 1.  5.  4.]\n",
            " [ 1.  1.  0.]\n",
            " [ 1.  1.  1.]\n",
            " [ 1.  8.  5.]\n",
            " [ 1.  7.  3.]\n",
            " [ 1.  7.  2.]\n",
            " [ 1.  1.  2.]\n",
            " [ 1.  1.  0.]\n",
            " [ 1.  7.  4.]\n",
            " [ 1.  1.  2.]\n",
            " [ 1. 14.  7.]\n",
            " [ 1.  3.  3.]\n",
            " [ 1.  0.  1.]\n",
            " [ 1.  3.  4.]\n",
            " [ 1.  2.  2.]\n",
            " [ 1.  8.  3.]\n",
            " [ 1.  0.  0.]\n",
            " [ 1.  0.  0.]\n",
            " [ 1.  0.  1.]\n",
            " [ 1.  1.  1.]\n",
            " [ 1.  2.  1.]\n",
            " [ 1.  1.  0.]\n",
            " [ 1.  5.  3.]\n",
            " [ 1.  1.  1.]\n",
            " [ 1. 22. 15.]\n",
            " [ 1.  1.  0.]\n",
            " [ 1.  2.  0.]\n",
            " [ 1.  5.  3.]\n",
            " [ 1.  6.  0.]\n",
            " [ 1.  2.  0.]\n",
            " [ 1.  2.  3.]\n",
            " [ 1.  0.  0.]\n",
            " [ 1.  5.  5.]\n",
            " [ 1.  0.  0.]\n",
            " [ 1.  2.  4.]\n",
            " [ 1.  0.  0.]\n",
            " [ 1.  1.  1.]\n",
            " [ 1.  0.  0.]\n",
            " [ 1.  2.  1.]\n",
            " [ 1.  2.  2.]\n",
            " [ 1.  0.  0.]\n",
            " [ 1.  0.  0.]\n",
            " [ 1. 12. 11.]\n",
            " [ 1.  0.  0.]\n",
            " [ 1.  0.  0.]\n",
            " [ 1.  1.  1.]\n",
            " [ 1.  4.  2.]\n",
            " [ 1.  5.  3.]\n",
            " [ 1.  0.  0.]\n",
            " [ 1.  0.  0.]\n",
            " [ 1. 16.  8.]\n",
            " [ 1.  4.  2.]\n",
            " [ 1.  0.  1.]\n",
            " [ 1.  1.  0.]\n",
            " [ 1.  0.  0.]\n",
            " [ 1.  0.  0.]\n",
            " [ 1.  0.  0.]\n",
            " [ 1.  2.  0.]\n",
            " [ 1.  0.  0.]\n",
            " [ 1.  0.  1.]\n",
            " [ 1.  0.  0.]\n",
            " [ 1.  0.  0.]\n",
            " [ 1.  1.  0.]\n",
            " [ 1.  0.  1.]\n",
            " [ 1.  2.  1.]\n",
            " [ 1.  0.  0.]\n",
            " [ 1.  0.  2.]\n",
            " [ 1.  1.  0.]\n",
            " [ 1.  5.  3.]\n",
            " [ 1.  0.  0.]\n",
            " [ 1.  4.  3.]\n",
            " [ 1.  1.  1.]\n",
            " [ 1.  1.  1.]\n",
            " [ 1.  0.  0.]\n",
            " [ 1.  2.  3.]\n",
            " [ 1.  3.  3.]\n",
            " [ 1.  4.  1.]\n",
            " [ 1.  3.  0.]\n",
            " [ 1.  1.  0.]\n",
            " [ 1.  4.  2.]\n",
            " [ 1.  0.  0.]\n",
            " [ 1.  0.  0.]\n",
            " [ 1.  3.  2.]\n",
            " [ 1.  6.  3.]\n",
            " [ 1.  0.  0.]\n",
            " [ 1.  3.  1.]\n",
            " [ 1.  0.  1.]\n",
            " [ 1.  1.  0.]\n",
            " [ 1.  0.  0.]\n",
            " [ 1.  4.  2.]\n",
            " [ 1.  2.  4.]\n",
            " [ 1.  2.  1.]\n",
            " [ 1.  0.  0.]\n",
            " [ 1.  5.  3.]\n",
            " [ 1.  5.  3.]\n",
            " [ 1.  6.  2.]\n",
            " [ 1.  1.  1.]\n",
            " [ 1.  0.  0.]\n",
            " [ 1.  4.  4.]\n",
            " [ 1.  0.  0.]\n",
            " [ 1.  4.  1.]\n",
            " [ 1.  2.  1.]\n",
            " [ 1.  0.  0.]\n",
            " [ 1.  0.  0.]\n",
            " [ 1.  1.  0.]\n",
            " [ 1.  1.  0.]\n",
            " [ 1.  0.  0.]\n",
            " [ 1.  2.  0.]\n",
            " [ 1.  2.  4.]\n",
            " [ 1.  0.  0.]\n",
            " [ 1.  4.  0.]\n",
            " [ 1.  1.  3.]\n",
            " [ 1.  0.  0.]\n",
            " [ 1.  0.  0.]\n",
            " [ 1.  1.  0.]\n",
            " [ 1.  4.  2.]\n",
            " [ 1.  1.  0.]\n",
            " [ 1.  2.  2.]\n",
            " [ 1.  0.  0.]\n",
            " [ 1.  0.  1.]\n",
            " [ 1.  0.  1.]\n",
            " [ 1.  0.  0.]\n",
            " [ 1.  0.  0.]\n",
            " [ 1.  0.  0.]\n",
            " [ 1.  1.  2.]\n",
            " [ 1.  4.  2.]\n",
            " [ 1.  4.  2.]\n",
            " [ 1.  0.  0.]\n",
            " [ 1.  0.  1.]\n",
            " [ 1.  3.  0.]\n",
            " [ 1.  1.  1.]\n",
            " [ 1.  3.  4.]\n",
            " [ 1.  0.  0.]\n",
            " [ 1.  5.  1.]\n",
            " [ 1.  0.  0.]\n",
            " [ 1.  1.  0.]\n",
            " [ 1.  0.  1.]\n",
            " [ 1.  3.  1.]\n",
            " [ 1.  0.  0.]\n",
            " [ 1.  0.  0.]\n",
            " [ 1.  0.  0.]\n",
            " [ 1.  2.  3.]\n",
            " [ 1.  1.  0.]\n",
            " [ 1.  5.  0.]\n",
            " [ 1.  6.  4.]\n",
            " [ 1.  4.  2.]\n",
            " [ 1.  0.  0.]\n",
            " [ 1.  1.  2.]\n",
            " [ 1.  6.  5.]\n",
            " [ 1.  0.  0.]\n",
            " [ 1.  0.  0.]\n",
            " [ 1.  2.  3.]\n",
            " [ 1.  1.  1.]\n",
            " [ 1.  0.  0.]\n",
            " [ 1.  3.  2.]\n",
            " [ 1.  2.  1.]\n",
            " [ 1.  1.  2.]\n",
            " [ 1.  0.  0.]\n",
            " [ 1.  1.  0.]\n",
            " [ 1.  1.  1.]\n",
            " [ 1.  0.  0.]\n",
            " [ 1.  5.  2.]\n",
            " [ 1.  2.  0.]\n",
            " [ 1.  4.  2.]\n",
            " [ 1.  5.  3.]\n",
            " [ 1.  1.  3.]\n",
            " [ 1.  3.  1.]\n",
            " [ 1.  0.  1.]\n",
            " [ 1.  1.  1.]\n",
            " [ 1.  1.  1.]\n",
            " [ 1.  2.  0.]\n",
            " [ 1.  0.  0.]\n",
            " [ 1.  0.  0.]\n",
            " [ 1.  2.  0.]\n",
            " [ 1.  0.  0.]\n",
            " [ 1.  1.  0.]\n",
            " [ 1.  4.  4.]\n",
            " [ 1.  0.  0.]\n",
            " [ 1.  2.  3.]\n",
            " [ 1.  0.  0.]\n",
            " [ 1.  0.  0.]\n",
            " [ 1.  1.  2.]\n",
            " [ 1.  0.  0.]\n",
            " [ 1.  3.  1.]\n",
            " [ 1.  0.  0.]\n",
            " [ 1.  2.  0.]\n",
            " [ 1.  0.  0.]\n",
            " [ 1.  2.  1.]\n",
            " [ 1.  0.  0.]\n",
            " [ 1.  0.  0.]\n",
            " [ 1.  0.  0.]\n",
            " [ 1.  1.  2.]\n",
            " [ 1.  4.  0.]\n",
            " [ 1.  0.  0.]\n",
            " [ 1.  1.  1.]\n",
            " [ 1.  4.  4.]\n",
            " [ 1.  0.  0.]\n",
            " [ 1.  3.  5.]\n",
            " [ 1.  2.  3.]\n",
            " [ 1. 13.  8.]\n",
            " [ 1.  2.  3.]\n",
            " [ 1.  0.  0.]\n",
            " [ 1.  0.  0.]\n",
            " [ 1.  0.  0.]\n",
            " [ 1.  0.  0.]\n",
            " [ 1.  0.  0.]\n",
            " [ 1.  3.  2.]\n",
            " [ 1.  2.  1.]\n",
            " [ 1.  1.  0.]\n",
            " [ 1.  4.  0.]\n",
            " [ 1.  0.  1.]\n",
            " [ 1.  0.  0.]\n",
            " [ 1.  1.  0.]\n",
            " [ 1.  1.  0.]\n",
            " [ 1.  5.  2.]\n",
            " [ 1.  6.  7.]\n",
            " [ 1.  2.  3.]\n",
            " [ 1.  0.  0.]\n",
            " [ 1.  0.  0.]\n",
            " [ 1.  4.  5.]\n",
            " [ 1.  0.  1.]\n",
            " [ 1.  0.  0.]\n",
            " [ 1.  0.  0.]\n",
            " [ 1.  1.  0.]\n",
            " [ 1.  0.  1.]\n",
            " [ 1.  3.  1.]\n",
            " [ 1.  1.  2.]\n",
            " [ 1.  3.  1.]\n",
            " [ 1.  0.  0.]\n",
            " [ 1.  0.  0.]\n",
            " [ 1.  0.  0.]\n",
            " [ 1.  3.  0.]\n",
            " [ 1.  5.  7.]\n",
            " [ 1.  8.  4.]\n",
            " [ 1.  8.  6.]\n",
            " [ 1.  4.  2.]\n",
            " [ 1.  0.  0.]\n",
            " [ 1.  2.  5.]\n",
            " [ 1.  0.  0.]\n",
            " [ 1.  1.  0.]\n",
            " [ 1.  0.  0.]\n",
            " [ 1.  4.  3.]\n",
            " [ 1.  2.  0.]\n",
            " [ 1.  1.  1.]\n",
            " [ 1.  2.  1.]\n",
            " [ 1.  4.  0.]\n",
            " [ 1.  0.  0.]\n",
            " [ 1.  0.  0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "Uzhe4NH6VBz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svm_model = SVC(kernel='rbf')\n",
        "\n",
        "# Fit the model to the training data\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the training data\n",
        "y_pred = svm_model.predict(X_test)\n"
      ],
      "metadata": {
        "id": "b7GsZ-p1VkYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Training Accuracy:\", train_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IsCK_ljgVtTJ",
        "outputId": "0685f0a3-a01b-422f-feb6-33d2d258442d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy: 0.6172839506172839\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "true_pos=0\n",
        "false_pos=0\n",
        "true_neg=0\n",
        "false_neg=0\n",
        "for i in range(len(y_test)):\n",
        "  if(y_test[i]==1) and (y_pred[i]==1):\n",
        "    true_pos=true_pos+1\n",
        "  if(y_test[i]==0) and (y_pred[i]==1):\n",
        "    false_pos=false_pos+1\n",
        "  if(y_test[i]==0) and (y_pred[i]==0):\n",
        "    true_neg=true_neg+1\n",
        "  if(y_test[i]==1) and (y_pred[i]==0):\n",
        "    false_neg=false_neg+1\n",
        "print(f\"true_pos={true_pos/len(y_test)} false_pos={false_pos/len(y_test)} true_neg={true_neg/len(y_test)} false neg={false_neg/len(y_test)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97AHKOrxXLQ5",
        "outputId": "2ff49aa9-a621-4a01-fdc7-9281db049bda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "true_pos=0.6172839506172839 false_pos=0.38271604938271603 true_neg=0.0 false neg=0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KMloVEcfaLy3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}